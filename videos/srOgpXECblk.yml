# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

category: conference
tags:
    - 'distributed systems'
    - web
title: 'Google I/O 2009 - Transactions Across Datacenters..'
language: English
recordingDate: 1243969586
description: "Google I/O 2009 - Transactions Across Datacenters (and Other Weekend Projects)\r\n\r\nRyan Barrett\r\n\r\n -- Contents --\r\n 0:55 - Background quotes\r\n 2:30 - Introduction: multihoming for read/write structured storage\r\n 5:12 - Three types of consistency: weak, eventual, strong\r\n 10:00 - Transactions: definition, background\r\n 12:22 - Why multihome? Why try do anything across multiple datacenters?\r\n 15:30 - Why *not* multihome?\r\n 17:45 - Three kinds of multihoming: none, some, full\r\n 27:35 - Multihoming techniques and how to evaluate them\r\n 28:30 - Technique #1: Backups\r\n 31:39 - Technique #2: Master/slave replication\r\n 35:42 - Technique #3: Multi-master replication\r\n 39:30 - Technique #4: Two phase commit\r\n 43:53 - Technique #5: Paxos\r\n 49:35 - Conclusion: no silver bullet. Embrace the tradeoffs!\r\n 52:15 - Questions\r\n -- End --\r\n\r\nIf you work on distributed systems, you try to design your system to keep running if any single machine fails. If you're ambitious, you might extend this to entire racks, or even more inconvenient sets of machines. However, what if your entire datacenter falls off the face of the earth?  This talk will examine how current large scale storage systems handle fault tolerance and consistency, with a particular focus on the App Engine datastore. We'll cover techniques such as replication, sharding, two phase commit, and consensus protocols (e.g. Paxos), then explore how they can be applied across datacenters.\r\n\r\nFor presentation slides and all I/O sessions, please go to: code.google.com/events/io/sessions.html"
