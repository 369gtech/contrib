# Contribution guide: https://github.com/watch-devtube/contrib

# speaker:
#   name: _____
#   twitter: _____ # mandatory twitter handle; must not include @

# tags: [____, _____, _____]

title: 'Two Case Studies in Private Data Analysis'
language: English
recordingDate: 1613506163
description: "A Google TechTalk, presented by Kamalika Chaudhuri, 2020/09/25\nPaper Title: \"New Case-Studies in Inferential and Differential Privacy\"\n\nABSTRACT:  The vast majority of computer science literature in privacy can be broadly divided into two categories --  differential, where the idea is to ensure that participation of an entity or an individual does not change the outcome significantly and inferential, where we are trying to bound the inferences an adversary can make based on auxiliary information.\n\nIn this talk, I will present two new case-studies, one in each framework. The first looks at a form of inferential privacy that allows more fine-grained control in a local setting than the individual level. The second looks at privacy against adversaries who have bounded learning capacity, and has ties to the theory of generative adversarial networks.\n\nJoint work with Joseph Geumlek, Jacob Imola and Ashwin Macchanavajhhala\n \nSpeaker Bio:   Kamalika Chaudhuri received a Bachelor of Technology degree in Computer Science and Engineering in 2002 from Indian Institute of Technology, Kanpur, and a PhD in Computer Science from University of California at Berkeley in 2007. After a postdoc at the Information Theory and Applications Center at UC San Diego, she joined the CSE department at UC San Diego as an assistant professor. She received an NSF CAREER Award in 2013 and a Hellman Faculty Fellowship in 2012.\n\nKamalika's research interests are in the foundations of trustworthy machine learning, which includes problems such as learning from sensitive data while preserving privacy, learning under sampling bias, and in the presence of an adversary. She is particularly interested in privacy-preserving machine learning, which addresses how to learn good models and predictors from sensitive data, while preserving the privacy of individuals. \n\""
