# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

tags:
    - ml
    - tensorflow
title: 'YOW! Data 2017 Joyce Wang - Covariate Shift - Challenges and Good Practice'
recordingDate: 1508454001
description: "A fundamental assumption in supervised machine learning is that both the training and query data are drawn from the same population/distribution. However, in real-life applications this is very often not the case as the query data distribution is unknown and cannot be guaranteed a-priori. Selection bias in collecting training samples will change the distribution of the training data from that of the overall population. This problem is known as covariate shift in the machine learning literature, and using a machine learning algorithm in this situation can result in spurious and often over-confident predictions.\n\nCovariate shift is only detectable when we have access to query data. Visualization of training and query data would be helpful to gain an initial impression. Machine learning models can be used to detect covariate shift. For example, Gaussian Process could model the similarity between each query point from feature space of training data. One-class SVMs could detect outliers of training data. Both strategies detect query points that live in a different domain of the feature space from the training dataset.\n\nWe suggest two strategies to mitigate covariate shift: re-weighting training data, and active learning with probabilistic models.\n\nFirst, re-weighting the training data is the process of matching distribution statistics between the training and query sets in feature space. When the model is trained (and validated) on re-weighted data, it is expected to generalise better to query data. However, significant overlap between training and query datasets is required.\n\nSecondly, there may be a situation where we can acquire the labels of a small portion of the query set, potentially at great expense, to reduce the effects of covariate shift. Probabilistic models are required in this case because they indicate the uncertainty in their prediction. Active learning enables us to optimally select small subsets of query points that aim to maximally shrink the uncertainty in our overall prediction. \n\nJoyce is an engineer at Data61 working in a team that deploys machine learning techniques to government sectors. Her expertise is spatial machine learning including imputation, data processsing, tensorflow and sklearn. She was awarded Bachelor of Science (Advanced Mathematics) from University of Sydney.\n\n\nFor more information on YOW! Data Conference, visit\nhttp://data.yowconference.com.au"
