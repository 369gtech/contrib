# Contribution guide: https://github.com/watch-devtube/contrib

# speaker:
#   name: _____
#   twitter: _____ # mandatory twitter handle; must not include @

# tags: [____, _____, _____]

title: 'DGL Operator: Distributed Graph Neural Network Training with DGL and K8s - Xiaoyu Zhai, Qihoo 360'
language: English
recordingDate: 1639859075
description: "Don’t miss out! Join us at our next event: KubeCon + CloudNativeCon Europe 2022 in Valencia, Spain from May 17-20. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.\n\nDGL Operator：基于 DGL 和 K8s 的分布式图神经网络训练控制器 | DGL Operator: Distributed Graph Neural Network Training with DGL and K8s - Xiaoyu Zhai, Qihoo 360\n\nDGL Operator（翟晓宇，奇虎 360）——许多学习任务需要处理包含元素间关系丰富信息的图形数据；工作流程的自动化和训练工作负载的细粒度管理可以使基于 DGL 的分布式图神经网络训练提高资源利用率，动态扩展各种 DGL 组件，降低分布式训练的系统复杂性，并应用机器学习操作。在本演示中，翟晓宇将首先通过一个图神经网络训练示例介绍图神经网络和 DGL 的背景，讨论执行 DGL 分布式训练的本地方式，以及它在生产规模集群中面临的挑战。稍后，翟晓宇将向观众展示 DGL Operator 解决方案的全貌，简要讨论如何使每个 DGL 组件成为容器化工作负载的概念，最后深入探讨 DGL Operator 的实现，包括多个分区选项和未来的设计。\n\nDGL Operator (Xiaoyu Zhai, Qihoo 360) – Many learning tasks require processing graph data that contains rich information about the relationships between elements; the automation of workflow and fine-grained management of training workload can enable DGL-based distributed GNN training to improve resource utilization, dynamic scaling of various DGL components, reduce system complexity of distributed training, and apply MLOps purposes. In this presentation, Xiaoyu Zhai will firstly go through a GNN training example to introduce the background of GNNs and DGL, talk about the native way to execute DGL distributed training, and the challenges in production-scale clusters it faces. Later on, Xiaoyu Zhai will give the audiences a big picture of DGL Operator solution, briefly discuss the abstraction that how to make each DGL component to be a containerized workload, and finally dive into the implementations of DGL Operator, including multiple partitioning options and future design."
