# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

category: conference
tags:
    - ml
    - web
speaker:
    name: 'Siraj Raval'
    twitter: sirajraval
    matches: [sirajraval]
title: 'Backpropagation Explained'
language: English
recordingDate: 1531088653
description: "The most popular optimization strategy in machine learning is called gradient descent. When gradient descent is applied to neural networks, its called back-propagation. In this video, i'll use analogies, animations, equations, and code to give you an in-depth understanding of this technique. Once you feel comfortable with back-propagation, everything else becomes easier. It uses calculus to help us update our machine learning models. Enjoy!\n\nCode for this video:\nhttps://github.com/llSourcell/backpropagation_explained\n\nPlease Subscribe! And like. And comment. That's what keeps me going. \n\nWant more education? Connect with me here:\nTwitter: https://twitter.com/sirajraval\nFacebook: https://www.facebook.com/sirajology\ninstagram: https://www.instagram.com/sirajraval\n\nThis video is apart of my Machine Learning Journey course:\nhttps://github.com/llSourcell/Machine_Learning_Journey\n\nMore learning resources:\nhttps://www.youtube.com/watch?v=XdM6ER7zTLk\nhttps://www.youtube.com/watch?v=nhqo0u1a6fw\nhttps://www.youtube.com/watch?v=jc2IthslyzM\nhttps://www.youtube.com/watch?v=IHZwWFHWa-w\nhttps://www.youtube.com/watch?v=umAeJ7LMCfU\nhttp://neuralnetworksanddeeplearning.com/chap2.html\n\nJoin us in the Wizards Slack channel:\nhttp://wizards.herokuapp.com/\n\nSign up for the next course at The School of AI:\nhttps://www.theschool.ai\n\nAnd please support me on Patreon:\nhttps://www.patreon.com/user?u=3191693"
