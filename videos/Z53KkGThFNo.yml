# Contribution guide: https://github.com/watch-devtube/contrib

# speaker:
#   name: _____
#   twitter: _____ # mandatory twitter handle; must not include @

# tags: [____, _____, _____]

title: 'Automated Machine Learning Performance Evaluation - Alejandro Saucedo'
language: English
recordingDate: 1621017782
description: "Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.\n\nAutomated Machine Learning Performance Evaluation - Alejandro Saucedo, The Institute for Ethical AI & Machine Learning\n\nOverview Deployed production machine learning models come on different sizes, shapes and flavours when deployed in cloud native infrastructure - each with varying hardware (and software) requirements. Whether it is RAM, CPU, GPU or Disk Space, there won't be an optimal global configuration for all your models' training and inference. In this talk we will cover the motivations and concepts around general benchmarking in software, as well as the key nuanced requirements to leverage these concepts in machine learning systems. We will learn about the theory behind benchmarking specifically on machine learning models, as well as the parameters that need to be accounted for, including latency, throughput, spikes, performance percentiles, outliers, between others. We will dive into a hands on example, where we will benchmark a model across multiple parameters to identify optimal performance on a specific hardware using Argo, Kubernetes and Seldon Core."
