# Contribution guide: https://github.com/watch-devtube/contrib

# speaker:
#   name: _____
#   twitter: _____ # mandatory twitter handle; must not include @

# tags: [____, _____, _____]

tags:
    - bigdata
    - performance
    - web
    - python
title: 'Ondrej Kokes - High Performance Data Loss | PyData Fest Amsterdam 2020'
language: English
recordingDate: 1599671089
description: "PyData is excited to announce PyData Global, November 11th - 15th! Tickets are now available: https://global.pydata.org/pages/tickets.html#pricing-and-ticket-purchases\nPart of an underrepresented group in tech? PyData Global is offering Diversity Scholarships. Applications close September 30th: https://docs.google.com/forms/d/e/1FAIpQLSfcFaTqVFjMa6kWlLPeEynEcbp1WrCxyxMGyyh4BP33eAviaA/viewform\n\n\nOndrej Kokes - High Performance Data Loss | PyData Fest Amsterdam 2020\n\nSay I give you a CSV with business orders, there are just three of them, the business is just taking off. Pandas tells you the average purchase is for $45, but you can equally use Excel, which gives you the same answer. In anticipation of a rosy future, you decide to use a Big Data Tool. You load it up and process the very same CSV and you report the business is actually doing thousands of orders for millions of dollars.\n\nWhat is this sorcery? Did I just hijack your analysis by “hacking” a CSV? Are big data tools broken? Can this happen in reality?\n\nWith the emergence of big-ish data, lots of tech has focused on the performance side of things. While more performance should be better than less performance, it’s not all roses. When evaluating these technologies, we’ve often stumbled upon integrity issues, sometimes to leading massive data losses. Or, as illustrated above, it can lead to strange data… gains.\n\nIn this talk, I’ll go over several implementation details that can lead to a large data loss without triggering any warnings or errors. We’ll go over a few cases (with data and code examples), but the main ideas are as follows:\n\ncorrectness should always trump performance - always make sure you’re getting the right results, only then focus on performance “explicit is better than implicit” - this Zen of Python quote is not just about code clarity, it’s useful when using data processing tools with hidden logic fail early, fail often - would you like a pipeline that never fails, but produces garbage, or would you prefer to be paged every now and then, because of an issue? The main takeaway is that you should understand the abstractions you’re using to process your data, no matter what your role is. Every single step that is there between a data source and your report/analysis/database/sink should be understandable, predictable, and, most importantly, correct. After all, our job as data guardians is to deliver data things reliably and correctly - our customer doesn’t care if it’s in the shiniest new Apache tool or in Cobol, orchestrated by Ada.\n\n===\nwww.pydata.org\r\n\r\nPyData is an educational program of NumFOCUS, a 501(c)3 non-profit organization in the United States. PyData provides a forum for the international community of users and developers of data analysis tools to share ideas and learn from each other. The global PyData network promotes discussion of best practices, new approaches, and emerging technologies for data management, processing, analytics, and visualization. PyData communities approach data science using many languages, including (but not limited to) Python, Julia, and R. \r\n\r\nPyData conferences aim to be accessible and community-driven, with novice to advanced level presentations. PyData tutorials and talks bring attendees the latest project features along with cutting-edge use cases."
