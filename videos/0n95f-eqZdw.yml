# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

category: conference
tags:
    - architecture
title: 'OpenAI Text Generator'
language: English
recordingDate: 1551193872
description: "OpenAI has the entire AI community debating its decision to not release the fully trained version of its powerful new text generator model dubbed GPT-2. I'm going to explain how GPT-2 works using code, math, and animations. We'll discuss its potential applications (both good and bad), ways of preventing misuse, and at the end of the video I'll give my take on whether OpenAI was justified in doing so. The transformer architecture is quickly replacing recurrent networks for sequence learning, and OpenAI's GPT-2 is the latest example of using it at scale. Enjoy! \n\nCode for this video: \nhttps://github.com/openai/gpt-2\n\nPlease Subscribe! And Like. And comment. Thats what keeps me going.\n\nWant more education? Connect with me here:\nTwitter: https://twitter.com/sirajraval\ninstagram: https://www.instagram.com/sirajraval\nFacebook: https://www.facebook.com/sirajology\n\nMore learning resources:\nhttps://medium.com/@asierarranz/i-have-created-a-website-to-query-the-gpt-2-openai-model-11dd30e1c8b0\nhttps://blog.openai.com/better-language-models/\nhttp://jalammar.github.io/illustrated-transformer/\nhttps://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XHVUts9KiLI\n\nWeb Demo of GPT-2:\nhttps://www.askskynet.com/\n\nGradient Descent:\nhttps://www.youtube.com/watch?v=XdM6ER7zTLk&t=774s\n\nFakebox:\nhttps://machinebox.io/docs/fakebox\n\nPrivacy tools:\nhttps://github.com/OpenMined/PySyft/tree/master/examples/tutorials\n\nJoin us at the School of AI:\nhttps://theschool.ai/\n\nJoin us in the Wizards Slack channel:\nhttp://wizards.herokuapp.com/\n\nPlease support me on Patreon:\nhttps://www.patreon.com/user?u=3191693\n\nSignup for my newsletter for exciting updates in the field of AI:\nhttps://goo.gl/FZzJ5w"
