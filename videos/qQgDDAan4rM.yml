# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata


tags:
    - storage
    - performance
    - web
    - ux
    - mobile
    - android
    - testing
    - javascript
title: 'GTAC 2009 - Automated Performance Test Data Collection and R'
language: English
recordingDate: 1257473256
description: "Google Tech Talk\r\nOctober 22, 2009\r\n\r\nABSTRACT\r\n\r\nAutomated Performance Test Data Collection and Reporting.  Presented by David Burns and David Henderson, smartFOCUS DIGITAL, at the 4th Annual Google Test Automation Conference, October 21st, 22nd, 2009, Zurich, CH \r\n\r\nWeb applications are growing in size and complexity with every new release. The addition of slightly more JavaScript and HTML can lead to the site becoming sluggish without an obvious cause. Fortunately more organizations are taking note of the correlation between site speed and profitability. Without suitable tools, developers are left stabbing in the dark to resolve performance issues until the application feels faster. Luckily there are many tools available, such as YSlow, that can help you through the process of measuring the performance of your application. However gathering this data manually can be time consuming, laborious and prone to human inconsistencies.  To illustrate the need for better performance information, we will outline the state of performance testing within the development cycle as carried out by many development teams around the world. We will then discuss the requirements for the system that has been created and implemented as a result of our research and development. The data gathering infrastructure, the tools used to create it and the scope of the data that is collected will be shown with solutions to problems encountered along the way. Performance statistics of a page, kept in a database, provide little useful information in isolation. When put into context with data from other pages and previous builds, the performance statistics suddenly become invaluable. To display this data in a digestible and comparable setting, a reporting portal will be demonstrated and its place within the development lifecycle explained. The final component of the system is the integration with the Tester's Heads-Up Display (T.H.U.D.). This is a plugin that aids rapid diagnosis and reporting of bugs by overlaying performance data on a specific page, as well as providing access to source control and bug tracking systems. This presentation will show how 'Automating Performance Test Data Collection and Reporting' has improved the awareness of web performance issues within our company. It has provided the evidence required to instigate changes and measure their impact. An average 75% reduction in primed page size has been achieved as a direct result of the system's introduction.\r\n\r\nBios: David Burns and David Henderson are both members of the development team at smartFOCUS DIGITAL, working on their SaaS solution. \r\n\r\nDavid Burns is the Lead Test Engineer, working on the web accessible parts of the system. He is the test automation champion for smartFOCUS and heads up the Test Design Authority within the group trying to find best practices in testing of smartFOCUS applications. David is an active blogger at http://www.theautomatedtester.co.uk\r\n\r\nDavid Henderson graduated from the University of Southampton with a first class Masters in Engineering in 2007. He is a developer working on the front end development mainly dealing with JavaScript and C#. David is currently tinkering with the Android platform in his spare time, looking to write the next killer app. \r\n\r\nBoth have an unhealthy obsession with measuring and improving the speed and weight of the user experience."
