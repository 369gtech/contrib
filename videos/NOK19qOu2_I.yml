# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

tags:
    - ml
    - devops
    - containers
title: 'Reducing Operational Barriers to Model Training'
recordingDate: 1554519653
description: "Speaker: Alexandra Johnson\nSlides: https://www.slideshare.net/SessionsEvents/alexandra-johnson-reducing-operational-barriers-to-model-training\n\nTalk Description: Technical hardware barriers to the size and complexity of models that can be trained are being constantly re-adjusted due to constant AI/ML hardware innovation. Reflecting this change, OpenAI recently highlighted that there was a 300,000x increase in compute from AlexNet in 2012 to AlphaGo Zero in 2017. As hardware innovation erodes these barriers, other barriers related to training, tuning, and operationalizing models are increasingly the bottleneck that keeps models from making it into production. \n\nThis particular talk focuses on the operational challenges that inhibit model training and tuning at scale. Most organizations have machine learning experts and devops experts, but few have individuals that possess both skills. As a result, there is a constant need for machine learning experts to rely on devops to spin up and manage clusters to train models, and for these devops professionals to extract as much context as possible from machine learning experts to do so in a way that is optimized for any given training job. At best, this process introduces friction in model development. At worst, it reduces the volume and variety of models that a team can train. \n\nApproach\nThis type of operational barrier to training and tuning models begs for a software rather than a hardware solution. There are a growing number of options available to teams to automate training orchestration and management. During this talk, Alexandra Johnson, Platform Engineering Lead of SigOpt, surveys a sampling of these methods and explains situations in which each is most valuable for certain modeling contexts. \n\nFollowing this survey, Alexandra will provide additional detail on methods that integrate training orchestration, model containerization and hyperparameter tuning. These approaches use training and tuning to inform each other, which creates a variety of benefits for a practitioner. She’ll then walk through the use of a command-line interface tool for performing containerized orchestration and recommend repositories where practitioners can learn more.\n\nResults\nAlexandra will explain the extent to which this approach is viable for different model types. She’ll also benchmark this approach against other popular methods to show the gains in team productivity and modeling throughput that result from a better approach to training orchestration. Se’ll then review how tuning can be impacted by this approach and share results relevant for the optimization of deep learning models. Finally, she’ll close out with a few insights on containers and predictions for how the use of containers will evolve in the coming years."
