# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata

tags:
    - ml
title: 'Learning 3D Models from a Single Still Image'
recordingDate: 1201950223
description: "Google Tech Talks\nJanuary, 29 2008\n\nABSTRACT\n\nWe present an algorithm to convert standard digital pictures into\n3-d models.\n\nThis is a challenging problem, since an image is formed by a projection of the 3-d scene onto two dimensions, thus losing the depth information. We take a supervised learning approach to this problem, and use a Markov Random Field (MRF) to model the image depth cues as well as the relationships between different parts of the image. We show that even on unstructured scenes (of indoor and outdoor environments, including forests, trees, buildings,\netc.), our algorithm is frequently able to recover fairly accurate 3-d models.\n\nWe use our method to create visually pleasing 3-d flythroughs from the\nimage. We also present a few extensions of these ideas, such as additionally incorporating triangulation (stereo) cues, and using multiple images to produce large scale 3-d models. We also apply our methods to two robotics applications: (a) high speed offroad obstacle avoidance on an autonomously driven remote-controlled car, and (b) having a robot unload items from a dishwasher.\n\nTo convert your own image of an outdoor scene, landscape, etc. to a 3-d model, please visit: http://make3d.stanford.edu\n\nJoint work with Min Sun and Andrew Y. Ng.\n\nSpeaker: Ashutosh Saxena\nAshutosh is a PhD candidate with Prof. Andrew Y. Ng in the Computer\nScience department in Stanford University.  He received his B. Tech.\nfrom Indian Institute of Technology (IIT Kanpur) in 2004.\n\nHis research focuses on machine learning approaches to problems in\ncomputer vision and in robotic manipulation.  Using data-driven machine\nlearning techniques, he developed algorithms for creating 3-d models from\na single image, and algorithms for robotic manipulation tasks such as\nopening doors, and grasping previously unseen objects."
