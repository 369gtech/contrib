# Editing guidelines: https://github.com/watch-devtube/contrib/#how-to-edit-video-metadata


title: 'ReLUs: An Alternative to the ReLU Activation Function - Wilder Rodrigues - Codemotion Amsterdam 2018'
language: English
recordingDate: 1530699322
description: 'In an attempt to reduce the impact caused by the Dying ReLU issue on Neural Networks and improve accuracy, the Rectified Linear Unit with Sigmoid (ReLUs) does not flatten out negative weights to zero, instead, it combines them with a sigmoid call and an extra hyper-parameter, thus enabling differentiability. This approach is also superior to the Leaky-ReLU implementation due to the oscillations that it adds to negative weights. it uses the difference between the sigmoid of sine and cosine functions to create a wave, thus deactivating some neurons during the forward pass, but not all of them.'
