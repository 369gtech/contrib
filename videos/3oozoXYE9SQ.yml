# Contribution guide: https://github.com/watch-devtube/contrib

# speaker:
#   name: _____
#   twitter: _____ # mandatory twitter handle; must not include @

# tags: [____, _____, _____]

title: 'FAST ''21 - The Dilemma between Deduplication and Locality: Can Both be Achieved?'
language: English
recordingDate: 1615589001
description: "FAST '21 - The Dilemma between Deduplication and Locality: Can Both be Achieved?\n\nXiangyu Zou and Jingsong Yuan, Harbin Institute of Technology, Shenzhen; Philip Shilane, Dell Technologies; Wen Xia, Harbin Institute of Technology, Shenzhen, and Wuhan National Laboratory for Optoelectronics; Haijun Zhang and Xuan Wang, Harbin Institute of Technology, Shenzhen\n\nData deduplication is widely used to reduce the size of backup workloads, but it has the known disadvantage of causing poor data locality, also referred to as the fragmentation problem, which leads to poor restore and garbage collection (GC) performance. Current research has considered writing duplicates to maintain locality (e.g. rewriting) or caching data in memory or SSD, but fragmentation continues to hurt restore and GC performance.\n\nInvestigating the locality issue, we observed that most duplicate chunks in a backup are directly from its previous backup. We therefore propose a novel management-friendly deduplication framework, called MFDedup, that maintains the locality of backup workloads by using a data classification approach to generate an optimal data layout. Specifically, we use two key techniques: Neighbor-Duplicate-Focus indexing (NDF) and Across-Version-Aware Reorganization scheme (AVAR), to perform duplicate detection against a previous backup and then rearrange chunks with an offline and iterative algorithm into a compact, sequential layout that nearly eliminates random I/O during restoration.\n\nEvaluation results with four backup datasets demonstrates that, compared with state-of-the-art techniques, MFDedup achieves deduplication ratios that are 1.12x to 2.19x higher and restore throughputs that are 2.63x to 11.64x faster due to the optimal data layout we achieve. While the rearranging stage introduces overheads, it is more than offset by a nearly-zero overhead GC process. Moreover, the NDF index only requires indexes for two backup versions, while the traditional index grows with the number of versions retained.\n\nView the full FAST '21 program at https://www.usenix.org/conference/fast21/technical-sessions"
